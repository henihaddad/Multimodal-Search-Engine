{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf_path = \"./2307.06435v9.pdf\"\n",
    "pdf_loader = PyPDFLoader(pdf_path)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "text_docs = text_splitter.split_documents(pdf_loader.load())l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "embeddings = OpenAIEmbeddings( model=\"text-embedding-3-small\")\n",
    "vector_store =  Chroma.from_documents(text_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "\n",
    "# model_name=\"openai/clip-vit-base-patch16\"\n",
    "# model = CLIPModel.from_pretrained(model_name)\n",
    "# processor = CLIPProcessor.from_pretrained(model_name)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# def find_top_matches(query_embedding, embeddings, top_k=3):\n",
    "#     # Calculate cosine similarities\n",
    "#     similarities = np.array([np.dot(query_embedding, emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(emb)) for emb in embeddings])\n",
    "#     # Get indices of top matches\n",
    "#     top_indices = np.argsort(similarities)[-3:][::-1]\n",
    "#     # Get top_k similarities\n",
    "#     top_similarities = similarities[top_indices]\n",
    "#     return list(zip(top_indices, top_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fitz\n",
    "import io\n",
    "from PIL import Image\n",
    "def extract_images_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        image_list = page.get_images(full=True)\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            images.append(image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_images = extract_images_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "# import open_clip\n",
    "# print(open_clip.list_pretrained())\n",
    "openclip_embeddings = OpenCLIPEmbeddings(model_name=\"ViT-B-16\",checkpoint=\"openai\")\n",
    "in_memory_images = extracted_images\n",
    "img_features = [openclip_embeddings.preprocess(image).unsqueeze(0) for image in in_memory_images]\n",
    "img_vector_store = np.array([openclip_embeddings.model.encode_image(feature).detach().numpy().squeeze(0) for feature in img_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def find_top_matches(text_features_np, img_vector_store, top_k=3):\n",
    "    similarity = cosine_similarity(text_features_np, img_vector_store)\n",
    "    similarity = similarity.squeeze(0)\n",
    "    top_indices = np.argsort(similarity)[-3:][::-1]\n",
    "    top_scores = similarity[top_indices]\n",
    "    top_images_scores = list(zip(top_indices, top_scores))\n",
    "    return top_images_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.27710183224840196), (7, 0.26018049791447523), (35, 0.24978073631841577)]\n",
      "[(Document(page_content='beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in', metadata={'page': 0, 'source': './2307.06435v9.pdf'}), 0.3075905835324745), (Document(page_content='beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in', metadata={'page': 0, 'source': './2307.06435v9.pdf'}), 0.3075905835324745), (Document(page_content='beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in', metadata={'page': 0, 'source': './2307.06435v9.pdf'}), 0.3075905835324745), (Document(page_content='beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in', metadata={'page': 0, 'source': './2307.06435v9.pdf'}), 0.3075905835324745), (Document(page_content='beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in', metadata={'page': 0, 'source': './2307.06435v9.pdf'}), 0.3075905835324745)]\n",
      "0.3075905835324745\n",
      "0.3075905835324745\n",
      "0.3075905835324745\n",
      "0.3075905835324745\n",
      "0.3075905835324745\n"
     ]
    }
   ],
   "source": [
    "# query = \"This section reviews LLMs, briefly describing their architectures, training objectives, pipelines, datasets, and fine-tuning details?\"\n",
    "# query=\"microsoft logo\"\n",
    "query = \"microsoft and the paper about LLms and all research paper about ?\"\n",
    "text_features = openclip_embeddings.embed_documents([query])\n",
    "text_features_np = np.array(text_features)\n",
    "\n",
    "\n",
    "top_3_images_indices_with_scores = find_top_matches(text_features_np, img_features_np, top_k=3)\n",
    "\n",
    "print(top_3_images_indices_with_scores)\n",
    "\n",
    "top_3_texts = vector_store.similarity_search_with_relevance_scores(query, k=5)\n",
    "\n",
    "print(top_3_texts)\n",
    "for txt in top_3_texts: print(txt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(page_content='beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in', metadata={'page': 0, 'source': './2307.06435v9.pdf'}), 0.3075905835324745), (Document(page_content='beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in', metadata={'page': 0, 'source': './2307.06435v9.pdf'}), 0.3075905835324745), (Document(page_content='beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in', metadata={'page': 0, 'source': './2307.06435v9.pdf'}), 0.3075905835324745), (Document(page_content='beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in', metadata={'page': 0, 'source': './2307.06435v9.pdf'}), 0.3075905835324745), (0, 0.27710183224840196), (7, 0.26018049791447523), (35, 0.24978073631841577)]\n",
      "top 3 results after reranking : [(Document(page_content='beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in', metadata={'page': 0, 'source': './2307.06435v9.pdf'}), 0.3075905835324745), (Document(page_content='beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in', metadata={'page': 0, 'source': './2307.06435v9.pdf'}), 0.3075905835324745), (Document(page_content='beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in', metadata={'page': 0, 'source': './2307.06435v9.pdf'}), 0.3075905835324745)]\n",
      "beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\n",
      "topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\n",
      "robotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in\n",
      "beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\n",
      "topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\n",
      "robotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in\n",
      "beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\n",
      "topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\n",
      "robotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "combined_results = top_3_images_indices_with_scores + top_3_texts\n",
    "# print(combined_results)\n",
    "combined_results.sort(key=lambda x: x[1], reverse=True)\n",
    "print(combined_results)\n",
    "top_3_results = combined_results[:3]\n",
    "print(f'top 3 results after reranking : {top_3_results}')\n",
    "\n",
    "vision_model = ChatOpenAI(temperature=0.5, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "final_results_that_will_be_feeded_to_gpt4 = []\n",
    "\n",
    "for i,result in enumerate(top_3_results):\n",
    "    # print(type(result[0]))\n",
    "    if isinstance(result[0], numpy.int64):\n",
    "        image = extracted_images[result[0]]\n",
    "        # images[result[0]].show()\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "        # send the image to gpt 4 vision\n",
    "        out = vision_model.invoke([SystemMessage(content=f'You should extract any relevant information to this query : {query} from the image')\n",
    "            ,HumanMessage(content=[\n",
    "            {\"type\": \"image\", \"image\": img_str}\n",
    "        ])])\n",
    "        print(out.content)\n",
    "        final_results_that_will_be_feeded_to_gpt4.append(out.content)\n",
    "        \n",
    "    else:\n",
    "        print(result[0].page_content)   \n",
    "        final_results_that_will_be_feeded_to_gpt4.append(result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in', 'beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in', 'beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in']\n",
      "Microsoft has been actively involved in the research and development of Large Language Models (LLMs). Their contributions span a wide range of topics, reflecting the broader trends in the field. Some key areas of focus include:\n",
      "\n",
      "1. **Architectural Innovations**: Microsoft has been exploring new architectures to improve the performance and capabilities of LLMs. This includes developing models that can handle more complex tasks and provide more accurate results.\n",
      "\n",
      "2. **Better Training Strategies**: Research on optimizing training processes to make them more efficient and effective is another area of interest. This includes techniques to reduce training time and computational resources while maintaining or improving model performance.\n",
      "\n",
      "3. **Context Length Improvements**: Enhancing the ability of LLMs to understand and generate longer contexts is crucial for many applications. Microsoft has been working on methods to extend the context length that models can handle.\n",
      "\n",
      "4. **Fine-Tuning**: Fine-tuning pre-trained models for specific tasks or domains is a significant area of research. This allows for more specialized and accurate applications of LLMs.\n",
      "\n",
      "5. **Multi-Modal LLMs**: Integrating multiple types of data (e.g., text, images, audio) into LLMs to create more versatile models is another research focus. This can lead to more comprehensive and context-aware AI systems.\n",
      "\n",
      "6. **Robotics**: Applying LLMs to control and interact with robotic systems is an emerging field. This involves using language models to interpret commands and generate actions in physical environments.\n",
      "\n",
      "7. **Datasets**: The creation and curation of large, high-quality datasets are essential for training effective LLMs. Microsoft has been involved in developing and sharing datasets that can be used for various research purposes.\n",
      "\n",
      "8. **Benchmarking**: Establishing benchmarks to evaluate the performance of LLMs is crucial for tracking progress and identifying areas for improvement. Microsoft contributes to the development of standardized benchmarks for the community.\n",
      "\n",
      "9. **Efficiency**: Improving the efficiency of LLMs, both in terms of computational resources and energy consumption, is a key area of research. This includes developing models that are faster and more cost-effective to deploy.\n",
      "\n",
      "Microsoft's research papers cover these topics and more, reflecting the rapid development and regular breakthroughs in the field of LLMs. Their work is part of a larger, vibrant research community that is continually pushing the boundaries of what these models can achieve.\n"
     ]
    }
   ],
   "source": [
    "print(final_results_that_will_be_feeded_to_gpt4)\n",
    "results_str = \"\\n\".join(final_results_that_will_be_feeded_to_gpt4)\n",
    "# send the final results to gpt 4\n",
    "gpt4_model = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=1024)\n",
    "out = gpt4_model.invoke([SystemMessage(content=f'use these contexts to respond to the user query {results_str}'),HumanMessage(content=query)])\n",
    "print(out.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
